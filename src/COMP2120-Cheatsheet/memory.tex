\section{Memory}
\emph{Memory Hierarchy}: Inbound (Registers, on-chip cache, cache, main mem)
$\rightarrow$ Outbound (disk, SSD, DVD)
$\rightarrow$ Off-line (magnetic tape)\\
\emph{Trends (top to bottom)}: Capacity $\uparrow$; Cost per bit $\downarrow$;
Access time $\uparrow$; Frequency of access $\downarrow$.\\
\emph{Principle of Locality}:
\textbf{Temporal} (recently accessed likely to be accessed again, e.g. \texttt{sum}) and
\textbf{Spatial} (items with nearby addresses likely to be accessed soon, e.g. \texttt{arr[]}).\\
\emph{Memory Organisation}:
\textbf{Big Endian} (left-to-right) and \textbf{Little Endian} (right-to-left).\\
\emph{Access Modes}: Sequential, Random, Associative.

\subsection*{Internal Memory}
\emph{ROM}: Read-only; Non-volatile; Written by masks; No erasure.\\
\emph{PROM}: Read-only; Non-volatile; Written electrically; No erasure.\\
\emph{EPROM}: Read-mostly; Non-volatile; Written electrically; Erased by UV light.\\
\emph{EEPROM}: Read-mostly; Non-volatile; Written electrically; Erased electrically (byte-wise).\\
\emph{Flash}: Read-mostly; Non-volatile; Written electrically; Erased electrically (block-wise); limited write cycles.\\
\emph{DRAM}: Read-write; Volatile; Use transistors; Refresh needed; Slow; Cheaper.\\
\emph{SRAM}: Read-write; Volatile; Use logic gates; No refresh; Fast; Expensive.

\subsection*{Bench-marking Memory Performance}
\emph{Access Time}: Time to read/write data.\\
\emph{Bandwidth/Transfer Rate}: Rate at which data can be read/written.\\
\emph{Memory Cycle Time}: Access time $+$ Transfer time.

\subsection*{Cache Memory}
A unit-addressable main memory with $n$-bit addresses, a block size of $2^k$ units, has $M=2^{n-k}$ blocks.
The cache has $m$ blocks (lines), $m \ll M$.

\subsubsection*{Address Mapping}

\emph{Direct Mapping}: 1-to-1 mapping.\\
$\text{(Cache line)} = \text{(Main mem block)}\,\%\,m$.\\
\textbf{Fields}: Tag (remaining bits), Line ($r$ bits, corresponds to $2^r$ lines), Offset ($k$ bits, corresponds to line size $2^k$ addressable units)\\
\textbf{Pros}:
\begin{enuminline}
    \item Simple circuitry.
    \item Fast.
\end{enuminline}\\
\textbf{Cons}:
\begin{enuminline}
    \item High miss rate.
\end{enuminline}

\emph{Fully Associative}: 1-to-all mapping.\\
\textbf{Fields}: Tag (remaining bits), Offset ($k$ bits, corresponds to line size $2^k$ addressable units)\\
\textbf{Pros}:
\begin{enuminline}
    \item Low miss rate.
    \item Flexible use of cache.
\end{enuminline}\\
\textbf{Cons}:
\begin{enuminline}
    \item Need to search all lines.
    \item Complex circuitry.
\end{enuminline}

\emph{Set Associative}: 1-to-some mapping.\\
$m \, (\#\text{ of lines}) = v \text{ sets} \times k \text{ lines/set}$.\\
$i \, (\text{Set \#}) = j \, (\text{Main mem block}) \, \%\, v$.\\
\textbf{Implementation}:
\begin{enuminline}
    \item $v$ associative caches. (high associativity)
    \item $k$ direct cache. ($k$-way set associative, low associativity)
\end{enuminline}\\
\textbf{Fields}: Tag (remaining bits), Set ($s$ bits, corresponds to $v=2^s$ sets), Offset ($k$ bits, corresponds to line size $2^k$ addressable units)\\
\textbf{Pros}:
\begin{enuminline}
    \item Low miss rate.
\end{enuminline}\\
\textbf{Cons}:
\begin{enuminline}
    \item Complex circuitry.
\end{enuminline}

\subsubsection*{Replacement Algorithms}
\emph{Random}: Randomly choose a line to replace. (Not used)\\
\emph{FIFO}: Replace the line that has been in the cache the longest.\\
\emph{LRU}: Replace the line that has been least recently used.\\
\emph{LFU}: Replace the line that has been least frequently used.\\
\textit{Not applicable to direct mapping}.

\subsubsection*{Write Policies}
\emph{Write-through}: Write every time cache is changed.\\
\emph{Write-back}: Write only when line is replaced.

\subsubsection*{Performance}
\emph{Average Access Time} $= \text{Hit time} + \text{Miss rate} \times \text{Miss penalty}$.

\subsubsection*{Unified/Split Cache}
\emph{Unified}: Instructions and data share the same cache. Auto balanced. Memory contention problem on pipeline and parallel executions, causes bottoleneck.\\
\emph{Split}: Instructions and data have fixed-size separate caches. Better performance. Main trend.

\subsection*{Virtual Memory}
\emph{Physical vs Logical Address}: Physical for addressing actual memory, space smaller; logical visible to the program, space may be larger.\\
\emph{Memory Management Unit (MMU)}: Maps between logical and physical addresses.

\subsubsection*{Paging}
\emph{Page vs Frame}: Page is a fixed-size block of logical memory, frame is a fixed-size block of physical memory.\\
\emph{Demand Paging}: Pages are loaded into memory only when needed.\\
\emph{Page Fault}: Occurs when a page is not in memory.\\
\textbf{Pros}: fast response; less memory usage;
\textbf{Cons}: page faults until stable set of pages loaded.\\
\emph{Page Table}: One page table per process, maps logical pages to physical frames.\\
\emph{PTE}: (\textbf{VPDF}) --
\textbf{V}alid bit (whether page in memory),
\textbf{P}rotection bits (manages access rights),
\textbf{D}irty bit (whether page modified),
\textbf{F}rame number (physical frame \#).\\
\emph{Translation Lookaside Buffer (TLB)}: Like a cache, stores some valid PTEs.
TLB consulted first, if not found, page table is consulted.\\

\subsection*{External Memory}

\subsubsection*{Hard Disk Drive (HDD)}

\emph{Components}: \textbf{Platter} (disk), \textbf{Track} (concentric circle), \textbf{Sector} (segment of track, 512 bytes), \textbf{Cylinder} (set of same tracks vertically).\\
\emph{Sector Format}: {\footnotesize\itshape e.g.}
\textbf{Gap 1} (separate sectors) -
\textbf{ID Field} (synch, track, head, sector \#, CRC) -
\textbf{Gap 2} (separate ID \& data) -
\textbf{Data Field} (data, CRC) -
\textbf{Gap 3}\\
\emph{Disk Layout Methods}: \begin{enuminline}
\item \emph{C}onstant \emph{A}ngular \emph{V}elocity -
easy read/write, density decreases towards the rim, wastes space;
\item \emph{M}ultiple \emph{Z}one \emph{R}ecording -
zones with different \# of sectors, maximise storage capacity, density similar but \textbf{NOT} uniform
\end{enuminline}\\
\emph{Data Access Time}: \begin{enuminline}
\item \textbf{Seek time} - move read/write head to cylinder, distance dependent, 5 - 15 ms startup, 0.2 - 1 ms consecutive;
\item \textbf{Rotational latency} - average is half a revolution;
\item \textbf{Transfer time} - $t_T\ll \text{seek}+\text{latency}$
$t_T = \frac{\text{bytes to transfer}}{\text{bytes per track}} \times \frac{1}{\text{rotation speed (rps)}}$
\end{enuminline}

\subsubsection*{Redundant Array of Independent Disks (RAID)}

\emph{0 (non-redundant)}: data stored in round-robin fashion,
efficient for accessing one block of data, no failure tolerance.\\
\emph{1 (mirroring)}: multi-disk failure tolerance,
either copy can be used $\rightarrow$ reduce seek time,
1 logical write = 2 physical writes.\\
\emph{2 (hamming code)}: not used, expensive,
$\text{\# redundant disks} \approx \log_2(\text{\# data disks})$,
efficient for parallel with small strip size, universally controlled spindles.\\
\emph{3 (bit-interleaved parity)}: 1 disk for parity, can recover from 1 disk failure
($p=b_\text{lost}\bxor b_2\bxor b_3 \Leftrightarrow b_\text{lost}=b_2\bxor b_3\bxor p$),
fast read/write, low ECC:Data ratio.\\
\emph{4 (block-level parity)}: not used, write penalty = 2 reads + 2 writes,
methods for writing: \begin{enuminline}
\item write data, recalculate parity, write parity;
\item write data, compare old data with new data, add difference to parity;
\end{enuminline}, individual spindle control, fast read, slow write, low ECC:Data.\\
\emph{5 (block-level distributed parity)}: 1 disk for parity, parity distributed across all disks,
can endure 1 disk failure, common for Network Attached Storage, fastest read/write, low ECC:Data\\
\emph{6 (dual redundancy)}: 2 disks for parity, can endure 2 disk failures,
use two different parity methods, distributed across all disks.